{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsers para Gramáticas independientes de contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requerimientos técnicos\n",
    "- Python 3\n",
    "- nltk\n",
    "- matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La diferencia entre NLTK, Matplotlib y las demás librerías que vamos a usar en esta clase (re, os, sys), es que a las dos primeras las debemos intstalar, en cambio las demás son provistas por python cuando lo intalamos.\n",
    "Veamoslas por dentro antes de importarlas en nuestro espacio de trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os, sys\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "Natural Language Toolkit es un conjunto de herramientas para crear programas en python que trabajen con lenguaje natural. nltk.org (la organización a cargo de crear y mantener esta librería) pone a disposición de manera online el [NLTK Book](https://www.nltk.org/book/), su libro especializado en el uso de la librería así como la explicación de conceptos generales de PLN.\n",
    "\n",
    "Para estar al día con los cambios en el código, lo mejor es chequear este libro on-line en vez de se versión editada, que puede traer ejemplos deprecados (\"nbest_parse\" vs. \"parse\")\n",
    "\n",
    "\n",
    "Al instalarnos esta librería, podemos acceder y hacer uso de sus funciones y clases para ayudar a construir nuestra propia herramienta.\n",
    "\n",
    "Por ejemplo, si quisieramos crear un pequeño corrector gramatical, podríamos utilizar alguno de los parsers de gramáticas independientes de contexto para identificar oraciones no gramaticales y devolver una alerta en este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gramáticas para NLTK\n",
    "\n",
    "Antes de pasar a los parsers, miremos las gramáticas que vamos a \"parsear\", construidas según lo pide el libro de Bird et al.\n",
    "[LINK A GRAMATICA](CFG.cfg)\n",
    "Notemos que la extensión del archivo es \".cfg\", así le vamos a avisar a nltk que la gramática debe ser entendida como \"Context Free Grammar\".\n",
    "Ahora podemos mirar por adentro que la grámatica cuenta con todos los elementos que, según lo que vimos en la clase, constituyen el formalismo de una CFG:\n",
    "\n",
    "Un axioma: O\n",
    "Símbolos no terminales: SN, PRO, NP, etc.\n",
    "Símbolos terminales: martín, cata, etc.\n",
    "Reglas de reescritura: cada una de las líneas de la gramática, que deben indicar que un elemento a la izquierda del signo -> se debe reescribir como los elementos a la derecha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí, accedamos a los distintos parsers y al finalizar podemos concretar el pequeño corrector gramatical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Descent Parser\n",
    "\n",
    "Este parser es de tipo **top-down** (analiza de arriba hacia abajo). Es decir que parte del símbolo de inicio y aplica las reglas de la gramática para obtener los constituyentes inmediatos y armar el árbol hasta llegar a los símbolos terminales. \n",
    "\n",
    "Debe chequear que los símbolos terminales coincidan con la secuencia del input sin haberla visto de antemano. Si no hay coincidencia, tiene que retroceder y buscar diferentes alternativas de parseo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antes que nada, qué es la recursión?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasemos al parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Descent Parser\n",
    "\n",
    "def rd_parser(oracion, gramatica):                  # Definimos una función llamada rd_parser con dos argumentos.\n",
    "    oracion = oracion.lower()                       # Convertimos a minúscula la oración utilizando una función nativa de la cadena de caracteres: lower(). \n",
    "        \n",
    "    if oracion.endswith('.'):                       # Otra función nativa de las strings nos ayuda a chequear si la cadena termina en x argumento.\n",
    "        oracion = re.sub('\\.',' ',oracion)          # En este caso, si la oración termina con un punto, se lo quita utilizando la librería de expresiones regulares \"re\".\n",
    "    else:                                           # Si no termina con un punto, \n",
    "        oracion = oracion                           # toma la oración como estaba originalmente.\n",
    "    lista_palabras = oracion.split()              # Dividimos la oración en palabras tomando como separador el espacio en blanco  con otra función nativa de las strings: split.\n",
    "    print(\"- Esta es la lista de palabras resultante: \", lista_palabras) # Split nos devuelve una lista (ordenada) de strings.\n",
    "      \n",
    "    gramatica = nltk.data.load(gramatica)           # Usamos la función de la sub librería \"data\" que nos permite cargar una gramática para que pueda ser usada luego por el parser.    \n",
    "    rd_parser = nltk.RecursiveDescentParser(gramatica) # Instanciamos la clase del parser que nos da NLTK pasandole un argumento obligatorio: la gramática.\n",
    "    for arbol in rd_parser.parse(lista_palabras):    # Una vez que instanciamos la clase, podemos usar sus funciones mientras le pasemos los argumentos requeridos. En este caso, usamos la función \"parser\" a la que le pasaremos nuestra lista de palabras, y la función nos devolverá cada árbol posible en mi gramática para esa oración.\n",
    "        print(\"- Este es el árbol resultante: \", arbol) # Imprimimos cada árbol en la consola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para correr el Recursive Descent Parser\n",
    "\n",
    "print('Escribí una oración:')                          # Para que me pida que escriba una oración\n",
    "oracion1 = input()                                     # Para que me abra un campo en el que escriba la oración\n",
    "gramatica = 'gramaticas/CFG.cfg'                       # Indicamos el path a nuestra gramatica\n",
    "rd_parser(oracion1, gramatica)                         # Llamamos a la función que creamos con los dos argumentos que establecimos como obligatorios.\n",
    "\n",
    "# Oraciones que acepta la gramática: \n",
    "# Cata/Martín/Julia/Maca/Pablo fuma\n",
    "# Cata/Martín/Julia/Maca/Pablo entregó/envió el/la/un/una plaza/facultad/regalo/globo/tabaco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algunas limitaciones del recursive descent parser\n",
    "\n",
    "- 1. La recursión a la izquierda provoca un loop infinito. (SN -> PRO | SN NP)\n",
    "- 2. El parser puede llegar a tomar demasiado tiempo en considerar opciones que mirando la oración ya sabemos que no son posibles. (Fernando fuma)\n",
    "- 3. El movimiento de backtracking borra construcciones de consituyentes que podrían ser útiles para otras partes de la oración. (El cigarrillo fue fumado por la persona)\n",
    "\n",
    "Veamos todo esto en la demo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Demo del Right Descent Parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.app.rdparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift Reduce Parser\n",
    "\n",
    "Este parser, en cambio, es **botom-up**, es decir que parte de la secuencia de palabras que conforman la oración a parsear y busca asignarle una estructura acorde con la gramática. Es decir que busca secuencias de palabras que coincidan con el lado derecho de las producciones de la gramática para reemplazarlas por el símbolo del lado izquierdo.\n",
    "\n",
    "Por ejemplo, si encuentra la secuencia \"fuma\" y en la gramática posee la regla V -> \"fuma\", hará el reemplazo por el símbolo V.\n",
    "\n",
    "Ahora bien, el parser intentará que la subsecuencia más larga posible coincida con los símbolos a la derecha, para ello usa un \"stack\" (una pila, donde se apilan cosas), una especie de memoria temporal donde acumula palabras de una secuencia, una a una, mientras intenta hacerlas coincidir con el lado derecho de una producción. Esta es la acción de \"shift\" (desplazamiento).\n",
    "\n",
    "Una vez que la subsecuencia coincide con una de las producciones, la reemplaza por el símbolo del lado izquierdo. Esta es la acción de \"reduce\" (reducir).\n",
    "\n",
    "\n",
    "El parser aplicará estos pasos hasta alcanzar el símbolo del axioma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift Reduce Parser\n",
    "\n",
    "def sr_parser(oracion, gramatica):                   # Definimos una función llamada sr_parser con dos argumentos.\n",
    "    oracion = oracion.lower()\n",
    "    if oracion.endswith('.'):\n",
    "        oracion = re.sub('\\.',' ',oracion)\n",
    "    else:\n",
    "        oracion = oracion\n",
    "    lista_palabras = oracion.split()\n",
    "    gramatica = nltk.data.load(gramatica)\n",
    "    sr_parser = nltk.ShiftReduceParser(gramatica)    # Instanciamos otra clase de parser\n",
    "    for arbol in sr_parser.parse(lista_palabras):\n",
    "        print(\"- Este es el árbol resultante: \", arbol)\n",
    "        #return(arbol)                                # Hacemos un retorno para la función, es decir que la función aquí se va a terminar, lo que nos cortara el loop, pero nos dibujará el árbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Escribí una oración:')\n",
    "oracion2 = input()\n",
    "gramatica = 'gramaticas/CFG.cfg'\n",
    "sr_parser(oracion2, gramatica)   \n",
    "\n",
    "# Oraciones que acepta la gramática: \n",
    "# Cata/Martín/Julia/Maca/Pablo fuma\n",
    "# Cata/Martín/Julia/Maca/Pablo entregó/envió el/la/un/una plaza/facultad/regalo/globo/tabaco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero qué son esos Warnings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algunas limitaciones del shift reduce parser\n",
    "\n",
    "- 1. Solo puede devolver un árbol posible, aunque la oración sea ambigua y acepte más de una estructura.\n",
    "- 2. En cada acción de reducir, debe seleccionar una, aunque haya más de una posible. Y si la posibilidad de hacer shift o reduce es ambivalente, deberá decidir por una de las dos acciones. Fallas en estas decisiones pueden resultar en una falla del parseo y, al no tener implementada una forma de backtracking, si siguió un camino que fue infructuoso, decidirá que esa oración no tiene solución. (Fernando fuma el cigarrillo en el parque)\n",
    "\n",
    "\n",
    "Veamos todo esto en la demo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Demo del Shift and Reduce parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.app.srparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart Parser\n",
    "\n",
    "Los parsers que vimos hasta acá tienen deficiencias, sea en eficiencia o completitud. El chart parser usa dynamic programming para ser más eficiente. Dynamic programming es una técnica para desarrollar algoritmos que tiende a solucionar un problema subdividiendolo en sub problemas. Consiste en guardar la solución a esos sub problemas para poder reusarla cada vez que se la necesita.\n",
    "\n",
    "El chart parser aplica esta técnica. Por ejemplo, construirá el SP \"con el telescopio\" una vez y lo guardará en una tabla. Esta tabla se denomina WFST (tabla de subcadenas bien formadas).\n",
    "\n",
    "Vamos a armar una:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_wfst(tokens, grammar):    \n",
    "    numtokens = len(tokens)    \n",
    "    wfst = [[None for i in range(numtokens+1)] for j in range(numtokens+1)]    # Esta forma de escribir un loop se llama \"list comprehension\"\n",
    "    for i in range(numtokens):        \n",
    "        productions = grammar.productions(rhs=tokens[i])        \n",
    "        wfst[i][i+1] = productions[0].lhs()    \n",
    "    return wfst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_wfst(wfst, tokens, grammar, trace=False):    \n",
    "    index = dict((p.rhs(), p.lhs()) for p in grammar.productions())    \n",
    "    numtokens = len(tokens)    \n",
    "    for span in range(2, numtokens+1):        \n",
    "        for start in range(numtokens+1-span):           \n",
    "            end = start + span            \n",
    "            for mid in range(start+1, end):                \n",
    "                nt1, nt2 = wfst[start][mid], wfst[mid][end]                \n",
    "                if nt1 and nt2 and (nt1,nt2) in index:                    \n",
    "                    wfst[start][end] = index[(nt1,nt2)]                    \n",
    "                    if trace:                        \n",
    "                        print(\"[%s] %3s [%s] %3s [%s] ==> [%s] %3s [%s]\" % (start, nt1, mid, nt2, end, start, index[(nt1,nt2)], end))    \n",
    "    return wfst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(wfst, tokens):    \n",
    "    print('\\nWFST ' + ' '.join([(\"%-4d\" % i) for i in range(1, len(wfst))]))    \n",
    "    for i in range(len(wfst)-1):        \n",
    "        print(\"%d   \" % i, end=' ')        \n",
    "        for j in range(1, len(wfst)):            \n",
    "            print(\"%-4s\" % (wfst[i][j] or '.'), end=' ')        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracion = \"fernando fuma en el balcon\".split()\n",
    "for indice in range(len(oracion)):\n",
    "    print(indice, oracion[indice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_gramatica = nltk.CFG.fromstring(\n",
    "    \"\"\"O -> SN SV\n",
    "    SP -> P SN\n",
    "    SN -> Det NC | 'fernando'\n",
    "    SV -> V NP | V SP\n",
    "    Det -> 'el'\n",
    "    NC -> 'balcon'\n",
    "    V -> 'fuma'\n",
    "    P -> 'en'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfst0 = init_wfst(oracion, chart_gramatica)\n",
    "display(wfst, oracion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfst1 = complete_wfst(wfst0, oracion, chart_gramatica, trace=True)\n",
    "display(wfst1, oracion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el resultado de correr el Chart Parser por una oración con nuestra gramática original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramatica = 'gramaticas/CFG.cfg'\n",
    "gramatica = nltk.data.load(gramatica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = nltk.ChartParser(gramatica)\n",
    "for tree in parser.parse(oracion):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Demo para el Chart Parser\n",
    "#nltk.app.chartparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bllip Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes hay que instalar el bllip parser. \n",
    "\n",
    "Para hacerlo, correr el siguiente comando en la terminal:\n",
    "\n",
    "    pip3 install --user bllipparser\n",
    "\n",
    "Brown Laboratory for Linguistic Information Processing\n",
    "\n",
    "Introduce gramáticas a partir de un corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLLIP Parser\n",
    "\n",
    "Bllip parser es un \"reranking parser\", un parser que va a asignarle a una oración con más de un árbol posible, el árbol más probable. Es decir, que las porducciones de la gramática van a estar acompañadas de un valor de probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install --user bllipparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bllipparser\n",
    "from bllipparser import RerankingParser                             #Importa el parser\n",
    "from bllipparser.ModelFetcher import download_and_install_model     # Descarga e instala el \"modelo\"\n",
    "\n",
    "model_dir = download_and_install_model('WSJ', 'tmp/models')         #Crea una variable con el \"modelo\"\n",
    "rrp = RerankingParser.from_unified_model_dir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracion2 = \"john runs through the hill\"\n",
    "rrp.simple_parse(oracion2)\n",
    "#rrp.parse(oracion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrp.tag(oracion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Escribí una oración en inglés')\n",
    "oracion4 = input()\n",
    "rrp.simple_parse(oracion4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracion3 = \"No one saw him disembark in the unanimous night, no one saw the bamboo canoe sink into the sacred mud, but in a few days there was no one who did not know that the taciturn man came from the South\"\n",
    "structure = rrp.simple_parse(oracion3)\n",
    "print(structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = bllipparser.Tree(structure)\n",
    "prettytree = tree.pretty_string()\n",
    "sentenceroot = tree.label\n",
    "sentencespan = tree.span()\n",
    "print(tree)\n",
    "print(prettytree)\n",
    "print(sentenceroot)\n",
    "print(sentencespan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
